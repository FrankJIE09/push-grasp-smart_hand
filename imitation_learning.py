#!/usr/bin/env python3
"""
æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿ
ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç»“æœä½œä¸ºæ ·æ¿ï¼Œè¿‡æ»¤æ— ä»·å€¼çš„åŠ¨ä½œ
"""

import os
import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import gymnasium as gym
from typing import List, Tuple, Dict, Any
import pickle
from collections import deque
import matplotlib.pyplot as plt
from datetime import datetime

# å¯¼å…¥åŸæœ‰çš„ç¯å¢ƒ
from rl_screw_pushing import ScrewPushingEnv, load_config

def get_obs_array(obs):
    # å¦‚æœæ˜¯tupleï¼Œå–ç¬¬ä¸€ä¸ª
    if isinstance(obs, tuple):
        return np.asarray(obs[0], dtype=np.float32)
    return np.asarray(obs, dtype=np.float32)

class ValueFilter:
    """ä»·å€¼è¿‡æ»¤å™¨ï¼šè¿‡æ»¤æ‰æ²¡æœ‰äº§ç”Ÿä»·å€¼çš„åŠ¨ä½œ"""
    
    def __init__(self, min_reward_threshold=0.0, min_improvement_threshold=0.01):
        self.min_reward_threshold = min_reward_threshold
        self.min_improvement_threshold = min_improvement_threshold
        self.episode_rewards = deque(maxlen=100)  # è®°å½•æœ€è¿‘100ä¸ªepisodeçš„å¥–åŠ±
        
    def is_valuable_action(self, action: np.ndarray, reward: float, 
                          episode_reward: float, step_count: int) -> bool:
        """
        åˆ¤æ–­åŠ¨ä½œæ˜¯å¦æœ‰ä»·å€¼
        
        Args:
            action: åŠ¨ä½œå‘é‡
            reward: å½“å‰æ­¥éª¤çš„å¥–åŠ±
            episode_reward: å½“å‰episodeçš„æ€»å¥–åŠ±
            step_count: å½“å‰æ­¥éª¤æ•°
            
        Returns:
            bool: åŠ¨ä½œæ˜¯å¦æœ‰ä»·å€¼
        """
        # 1. å¥–åŠ±é˜ˆå€¼è¿‡æ»¤
        if reward < self.min_reward_threshold:
            return False
            
        # 2. å¥–åŠ±æ”¹è¿›è¿‡æ»¤
        if len(self.episode_rewards) > 0:
            avg_reward = np.mean(self.episode_rewards)
            if episode_reward < avg_reward + self.min_improvement_threshold:
                return False
                
        # 3. åŠ¨ä½œæœ‰æ•ˆæ€§è¿‡æ»¤ï¼ˆé¿å…é‡å¤æˆ–æ— æ•ˆåŠ¨ä½œï¼‰
        if np.linalg.norm(action) < 0.01:  # åŠ¨ä½œå¤ªå°
            return False
            
        return True
    
    def update_episode_reward(self, episode_reward: float):
        """æ›´æ–°episodeå¥–åŠ±è®°å½•"""
        self.episode_rewards.append(episode_reward)

class DemonstrationCollector:
    """æ¼”ç¤ºæ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, model_path: str, config: Dict[str, Any], 
                 value_filter: ValueFilter):
        self.model_path = model_path
        self.config = config
        self.value_filter = value_filter
        self.demonstrations = []
        
    def collect_demonstrations(self, num_episodes: int = 100, 
                             min_episode_length: int = 50) -> List[Dict]:
        """
        æ”¶é›†æœ‰ä»·å€¼çš„æ¼”ç¤ºæ•°æ®
        
        Args:
            num_episodes: æ”¶é›†çš„episodeæ•°é‡
            min_episode_length: æœ€å°episodeé•¿åº¦
            
        Returns:
            List[Dict]: æ¼”ç¤ºæ•°æ®åˆ—è¡¨
        """
        print(f"ğŸ”„ å¼€å§‹æ”¶é›†æ¼”ç¤ºæ•°æ®ï¼Œç›®æ ‡episodeæ•°: {num_episodes}")
        
        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        try:
            model = PPO.load(self.model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return []
        
        # åˆ›å»ºç¯å¢ƒ
        env = ScrewPushingEnv(config=self.config)
        env = DummyVecEnv([lambda: env])
        
        valuable_episodes = 0
        total_episodes = 0
        
        while valuable_episodes < num_episodes and total_episodes < num_episodes * 3:
            obs = env.reset()
            episode_data = {
                'observations': [],
                'actions': [],
                'rewards': [],
                'episode_reward': 0.0,
                'episode_length': 0
            }
            
            done = False
            step_count = 0
            
            while not done:
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
                action, _ = model.predict(obs, deterministic=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, info = env.step(action)
                
                # è®°å½•æ•°æ®
                episode_data['observations'].append(obs[0].copy())
                episode_data['actions'].append(action[0].copy())
                episode_data['rewards'].append(reward[0])
                episode_data['episode_reward'] += reward[0]
                episode_data['episode_length'] += 1
                
                obs = next_obs
                step_count += 1
                
                # æ£€æŸ¥episodeæ˜¯å¦è¿‡é•¿
                if step_count > 500:  # æœ€å¤§æ­¥æ•°é™åˆ¶
                    break
            
            total_episodes += 1
            
            # ä½¿ç”¨ä»·å€¼è¿‡æ»¤å™¨åˆ¤æ–­episodeæ˜¯å¦æœ‰ä»·å€¼
            if (episode_data['episode_length'] >= min_episode_length and
                self.value_filter.is_valuable_action(
                    np.array(episode_data['actions']), 
                    episode_data['episode_reward'],
                    episode_data['episode_reward'],
                    episode_data['episode_length']
                )):
                
                self.demonstrations.append(episode_data)
                valuable_episodes += 1
                self.value_filter.update_episode_reward(episode_data['episode_reward'])
                
                print(f"âœ… æ”¶é›†åˆ°æœ‰ä»·å€¼çš„episode {valuable_episodes}/{num_episodes}, "
                      f"å¥–åŠ±: {episode_data['episode_reward']:.3f}, "
                      f"é•¿åº¦: {episode_data['episode_length']}")
            else:
                print(f"âŒ è¿‡æ»¤æ‰æ— ä»·å€¼çš„episode {total_episodes}, "
                      f"å¥–åŠ±: {episode_data['episode_reward']:.3f}, "
                      f"é•¿åº¦: {episode_data['episode_length']}")
        
        print(f"ğŸ¯ æ¼”ç¤ºæ•°æ®æ”¶é›†å®Œæˆï¼")
        print(f"   æ€»episodeæ•°: {total_episodes}")
        print(f"   æœ‰ä»·å€¼episodeæ•°: {len(self.demonstrations)}")
        print(f"   è¿‡æ»¤ç‡: {(total_episodes - len(self.demonstrations)) / total_episodes * 100:.1f}%")
        
        return self.demonstrations

class DemonstrationDataset(Dataset):
    """æ¼”ç¤ºæ•°æ®é›†"""
    
    def __init__(self, demonstrations: List[Dict], sequence_length: int = 10):
        self.demonstrations = demonstrations
        self.sequence_length = sequence_length
        self.samples = self._prepare_samples()
        
    def _prepare_samples(self) -> List[Tuple]:
        """å‡†å¤‡è®­ç»ƒæ ·æœ¬"""
        samples = []
        
        for demo in self.demonstrations:
            obs = np.array(demo['observations'])
            actions = np.array(demo['actions'])
            
            # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
            for i in range(len(obs) - self.sequence_length + 1):
                obs_seq = obs[i:i + self.sequence_length]
                action_seq = actions[i:i + self.sequence_length]
                
                samples.append((obs_seq, action_seq))
        
        return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        obs_seq, action_seq = self.samples[idx]
        return (torch.FloatTensor(obs_seq), torch.FloatTensor(action_seq))

class ImitationNetwork(nn.Module):
    """æ¨¡ä»¿å­¦ä¹ ç½‘ç»œ"""
    
    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):
        super(ImitationNetwork, self).__init__()
        
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        
        # ç¼–ç å™¨ï¼šå°†è§‚å¯Ÿåºåˆ—ç¼–ç ä¸ºéšè—çŠ¶æ€
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # åŠ¨ä½œé¢„æµ‹å™¨ï¼šä»éšè—çŠ¶æ€é¢„æµ‹åŠ¨ä½œ
        self.action_predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1, 1]
        )
        
        # LSTMç”¨äºå¤„ç†åºåˆ—ä¿¡æ¯
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)
        
    def forward(self, obs_seq: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            obs_seq: è§‚å¯Ÿåºåˆ— [batch_size, sequence_length, obs_dim]
            
        Returns:
            torch.Tensor: é¢„æµ‹çš„åŠ¨ä½œåºåˆ— [batch_size, sequence_length, action_dim]
        """
        batch_size, seq_len, _ = obs_seq.shape
        
        # ç¼–ç è§‚å¯Ÿåºåˆ—
        encoded = self.encoder(obs_seq)  # [batch_size, sequence_length, hidden_dim]
        
        # LSTMå¤„ç†åºåˆ—
        lstm_out, _ = self.lstm(encoded)  # [batch_size, sequence_length, hidden_dim]
        
        # é¢„æµ‹åŠ¨ä½œ
        actions = self.action_predictor(lstm_out)  # [batch_size, sequence_length, action_dim]
        
        return actions

class ImitationTrainer:
    """æ¨¡ä»¿å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def train(self, demonstrations: List[Dict], 
              batch_size: int = 32, 
              learning_rate: float = 1e-4,
              num_epochs: int = 100,
              sequence_length: int = 10) -> ImitationNetwork:
        """
        è®­ç»ƒæ¨¡ä»¿å­¦ä¹ ç½‘ç»œ
        
        Args:
            demonstrations: æ¼”ç¤ºæ•°æ®
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            num_epochs: è®­ç»ƒè½®æ•°
            sequence_length: åºåˆ—é•¿åº¦
            
        Returns:
            ImitationNetwork: è®­ç»ƒå¥½çš„ç½‘ç»œ
        """
        print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡ä»¿å­¦ä¹ ç½‘ç»œ")
        print(f"   æ¼”ç¤ºæ•°æ®æ•°é‡: {len(demonstrations)}")
        print(f"   åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        
        # å‡†å¤‡æ•°æ®é›†
        dataset = DemonstrationDataset(demonstrations, sequence_length)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # è·å–è¾“å…¥ç»´åº¦
        sample_obs, sample_action = dataset[0]
        obs_dim = sample_obs.shape[-1]
        action_dim = sample_action.shape[-1]
        
        print(f"   è§‚å¯Ÿç»´åº¦: {obs_dim}")
        print(f"   åŠ¨ä½œç»´åº¦: {action_dim}")
        
        # åˆ›å»ºç½‘ç»œ
        network = ImitationNetwork(obs_dim, action_dim).to(self.device)
        optimizer = optim.Adam(network.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒè®°å½•
        train_losses = []
        
        # å¼€å§‹è®­ç»ƒ
        for epoch in range(num_epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            for batch_obs, batch_actions in dataloader:
                batch_obs = batch_obs.to(self.device)
                batch_actions = batch_actions.to(self.device)
                
                # å‰å‘ä¼ æ’­
                predicted_actions = network(batch_obs)
                
                # è®¡ç®—æŸå¤±
                loss = criterion(predicted_actions, batch_actions)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            avg_loss = epoch_loss / num_batches
            train_losses.append(avg_loss)
            
            if (epoch + 1) % 10 == 0:
                print(f"   Epoch {epoch + 1}/{num_epochs}, æŸå¤±: {avg_loss:.6f}")
        
        print(f"âœ… æ¨¡ä»¿å­¦ä¹ è®­ç»ƒå®Œæˆï¼")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curve(train_losses)
        
        return network
    
    def _plot_training_curve(self, losses: List[float]):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(losses)
        plt.title('Imitation Learning Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        
        # ä¿å­˜å›¾ç‰‡
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = f"imitation_learning_loss_{timestamp}.png"
        plt.savefig(plot_path)
        plt.close()
        
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")

class ImitationAgent:
    """æ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“"""
    
    def __init__(self, network: ImitationNetwork, sequence_length: int = 10):
        self.network = network
        self.sequence_length = sequence_length
        self.device = next(network.parameters()).device
        self.observation_buffer = deque(maxlen=sequence_length)
        
    def predict(self, observation: np.ndarray) -> np.ndarray:
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            observation: å½“å‰è§‚å¯Ÿ
            
        Returns:
            np.ndarray: é¢„æµ‹çš„åŠ¨ä½œ
        """
        obs = get_obs_array(observation)
        self.observation_buffer.append(obs)
        
        # å¦‚æœç¼“å†²åŒºæœªæ»¡ï¼Œè¿”å›é›¶åŠ¨ä½œ
        if len(self.observation_buffer) < self.sequence_length:
            return np.zeros(self.network.action_dim)
        
        # å‡†å¤‡è¾“å…¥åºåˆ—
        obs_seq = np.array(list(self.observation_buffer), dtype=np.float32)
        obs_tensor = torch.FloatTensor(obs_seq).unsqueeze(0).to(self.device)
        
        # é¢„æµ‹åŠ¨ä½œ
        with torch.no_grad():
            action_seq = self.network(obs_tensor)
            predicted_action = action_seq[0, -1].cpu().numpy()  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œ
        
        return predicted_action
    
    def reset(self):
        """é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€"""
        self.observation_buffer.clear()

def evaluate_imitation_agent(agent: ImitationAgent, config: Dict[str, Any], 
                           num_episodes: int = 10) -> Dict[str, float]:
    """
    è¯„ä¼°æ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“
    
    Args:
        agent: æ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“
        config: ç¯å¢ƒé…ç½®
        num_episodes: è¯„ä¼°episodeæ•°
        
    Returns:
        Dict[str, float]: è¯„ä¼°ç»“æœ
    """
    print(f"ğŸ” å¼€å§‹è¯„ä¼°æ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“")
    
    env = ScrewPushingEnv(config=config)
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(num_episodes):
        obs = env.reset()
        obs = get_obs_array(obs)
        agent.reset()
        
        episode_reward = 0.0
        step_count = 0
        
        done = False
        while not done and step_count < 500:
            action = agent.predict(obs)
            result = env.step(action)
            
            # å¤„ç†ä¸åŒç‰ˆæœ¬çš„ç¯å¢ƒè¿”å›å€¼
            if len(result) == 4:
                obs, reward, done, info = result
            elif len(result) == 5:
                obs, reward, done, truncated, info = result
                done = done or truncated
            else:
                print(f"âŒ æœªçŸ¥çš„ç¯å¢ƒstepè¿”å›å€¼æ ¼å¼: {result}")
                break
            
            obs = get_obs_array(obs)
            episode_reward += reward
            step_count += 1
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(step_count)
        
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸï¼ˆæ ¹æ®å¥–åŠ±é˜ˆå€¼ï¼‰
        if episode_reward > -15000:  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é˜ˆå€¼
            success_count += 1
        
        print(f"   Episode {episode + 1}: å¥–åŠ±={episode_reward:.3f}, æ­¥æ•°={step_count}")
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    avg_reward = np.mean(episode_rewards)
    avg_length = np.mean(episode_lengths)
    success_rate = success_count / num_episodes
    
    results = {
        'avg_reward': avg_reward,
        'avg_length': avg_length,
        'success_rate': success_rate,
        'min_reward': np.min(episode_rewards),
        'max_reward': np.max(episode_rewards),
        'std_reward': np.std(episode_rewards)
    }
    
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
    print(f"   å¹³å‡æ­¥æ•°: {avg_length:.1f}")
    print(f"   æˆåŠŸç‡: {success_rate:.1%}")
    print(f"   å¥–åŠ±èŒƒå›´: [{results['min_reward']:.3f}, {results['max_reward']:.3f}]")
    
    return results

def demo_imitation_agent_with_viewer(agent: ImitationAgent, config: dict, max_steps: int = 500):
    """
    ç”¨MuJoCo viewerå±•ç¤ºæ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“çš„æ¨ç†è¿‡ç¨‹
    """
    import time
    env = ScrewPushingEnv(config=config, render_mode='human')  # ç¡®ä¿render_mode='human'
    obs = env.reset()
    obs = get_obs_array(obs)
    agent.reset()
    done = False
    step_count = 0
    total_reward = 0.0

    while not done and step_count < max_steps:
        action = agent.predict(obs)
        result = env.step(action)
        if len(result) == 4:
            obs, reward, done, info = result
        elif len(result) == 5:
            obs, reward, done, truncated, info = result
            done = done or truncated
        else:
            print(f"âŒ æœªçŸ¥çš„ç¯å¢ƒstepè¿”å›å€¼æ ¼å¼: {result}")
            break
        obs = get_obs_array(obs)
        total_reward += reward
        step_count += 1
        time.sleep(0.03)  # æ§åˆ¶ä»¿çœŸé€Ÿåº¦ï¼Œ30~40msä¸€å¸§æ›´æµç•…

    print(f"æ¼”ç¤ºç»“æŸï¼Œæ€»æ­¥æ•°: {step_count}, æ€»å¥–åŠ±: {total_reward:.2f}")
    env.close()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ¨¡ä»¿å­¦ä¹ ç³»ç»Ÿ")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # åˆ›å»ºä»·å€¼è¿‡æ»¤å™¨ - è°ƒæ•´å‚æ•°ä»¥é€‚åº”å½“å‰æƒ…å†µ
    value_filter = ValueFilter(
        min_reward_threshold=-20000.0,  # å¤§å¹…é™ä½å¥–åŠ±é˜ˆå€¼
        min_improvement_threshold=-1000.0  # å…è®¸è´Ÿæ”¹è¿›
    )
    
    # æ”¶é›†æ¼”ç¤ºæ•°æ®
    collector = DemonstrationCollector(
        model_path="training_results/models/screw_pushing_agent",
        config=config,
        value_filter=value_filter
    )
    
    demonstrations = collector.collect_demonstrations(
        num_episodes=10,  # å‡å°‘ç›®æ ‡episodeæ•°
        min_episode_length=20  # å‡å°‘æœ€å°episodeé•¿åº¦
    )
    
    if not demonstrations:
        print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æœ‰ä»·å€¼çš„æ¼”ç¤ºæ•°æ®")
        return
    
    print(f"âœ… æˆåŠŸæ”¶é›†åˆ° {len(demonstrations)} ä¸ªæœ‰ä»·å€¼çš„æ¼”ç¤ºæ•°æ®")
    
    # è®­ç»ƒæ¨¡ä»¿å­¦ä¹ ç½‘ç»œ
    trainer = ImitationTrainer(config)
    network = trainer.train(
        demonstrations=demonstrations,
        batch_size=16,
        learning_rate=1e-4,
        num_epochs=30,  # å‡å°‘è®­ç»ƒè½®æ•°
        sequence_length=10
    )
    
    # åˆ›å»ºæ¨¡ä»¿å­¦ä¹ æ™ºèƒ½ä½“
    agent = ImitationAgent(network, sequence_length=10)
    
    # è¯„ä¼°æ™ºèƒ½ä½“
    results = evaluate_imitation_agent(agent, config, num_episodes=5)
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜ç½‘ç»œ
    torch.save(network.state_dict(), f"imitation_network_{timestamp}.pth")
    print(f"ğŸ’¾ ç½‘ç»œå·²ä¿å­˜: imitation_network_{timestamp}.pth")
    
    # ä¿å­˜æ¼”ç¤ºæ•°æ®
    with open(f"demonstrations_{timestamp}.pkl", 'wb') as f:
        pickle.dump(demonstrations, f)
    print(f"ğŸ’¾ æ¼”ç¤ºæ•°æ®å·²ä¿å­˜: demonstrations_{timestamp}.pkl")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open(f"imitation_results_{timestamp}.json", 'w') as f:
        json.dump(results, f, indent=2)
    print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: imitation_results_{timestamp}.json")
    
    # æ¼”ç¤ºæ™ºèƒ½ä½“
    demo_imitation_agent_with_viewer(agent, config, max_steps=500)

if __name__ == "__main__":
    main() 