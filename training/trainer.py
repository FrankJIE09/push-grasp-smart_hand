"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
"""

import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

from environment.screw_pushing_env import ScrewPushingEnv


class ScrewPushingTrainer:
    """èºä¸æ¨åŠ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, 
                 total_timesteps=10000,
                 save_freq=1000,
                 eval_freq=500,
                 model_save_path="screw_pushing_agent"):
        self.total_timesteps = total_timesteps
        self.save_freq = save_freq
        self.eval_freq = eval_freq
        self.model_save_path = model_save_path

    def create_env(self):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        def make_env():
            env = ScrewPushingEnv()
            env = Monitor(env)
            return env

        return DummyVecEnv([make_env])

    def create_eval_env(self):
        """åˆ›å»ºè¯„ä¼°ç¯å¢ƒ"""
        def make_eval_env():
            env = ScrewPushingEnv()
            env = Monitor(env)
            return env

        return DummyVecEnv([make_eval_env])

    def train(self):
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒèºä¸æ¨åŠ¨æ™ºèƒ½ä½“...")
        
        # åˆ›å»ºç¯å¢ƒ
        env = self.create_env()
        eval_env = self.create_eval_env()

        # åˆ›å»ºæ¨¡å‹
        model = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            tensorboard_log="./logs/"
        )

        # è®¾ç½®å›è°ƒå‡½æ•°
        checkpoint_callback = CheckpointCallback(
            save_freq=self.save_freq,
            save_path="./checkpoints/",
            name_prefix="screw_pushing_model"
        )

        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path="./best_model/",
            log_path="./logs/",
            eval_freq=self.eval_freq,
            deterministic=True,
            render=False
        )

        # å¼€å§‹è®­ç»ƒ
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
        print(f"   æ€»æ­¥æ•°: {self.total_timesteps}")
        print(f"   ä¿å­˜é¢‘ç‡: {self.save_freq}")
        print(f"   è¯„ä¼°é¢‘ç‡: {self.eval_freq}")
        print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {self.model_save_path}")

        model.learn(
            total_timesteps=self.total_timesteps,
            callback=[checkpoint_callback, eval_callback],
            progress_bar=True
        )

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model.save(self.model_save_path)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_save_path}")

        return model

    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if os.path.exists(model_path + ".zip"):
            model = PPO.load(model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return model
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None 